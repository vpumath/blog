[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi there, welcome to my math blog! I will write about topics that I want either to (re)learn, I am curious about or I find fun. My style will be expository writing, but I will add some mathematical details and terms to the text to make things somewhat rigorous (sometimes I can go full technical details mode). I hope that this provides a general overview of the topics for larger audience and an opportunity to dive into more advanced mathematics for the readers that get interested about more advanced terms, concepts and theorems.\nI write this blog as a hobby, so this blog ain’t peer reviewed. I have limited amount of energy and time to proofread and double check calculations and theorems. Check details and reasoning also yourself and from another sources. I do my best to catch every mistake before posting, but some errors can slip through despite my effort.\nHopefully you have enjoyable time while reading the blog! ^^"
  },
  {
    "objectID": "posts/Analytically_solvable_single_species_population_models_part_1/index.html",
    "href": "posts/Analytically_solvable_single_species_population_models_part_1/index.html",
    "title": "Analytically solvable single species population models part 1",
    "section": "",
    "text": "Mathematical biology is one of those topics that I want to learn so I will be writing about it. In this post, I will discuss shortly single species continuous population models that can be analytically solved. They may not have practical use cases nowadays anymore, but knowing what can be solved analytically is mathematically interesting and instructive topic.\n(There may be more mistakes and typos in this post since I have written this with little time and using my previous notes on the topic. I wanted to adjust my posting schedule to one post each Sunday and doing things this way I can move my posting schedule. I have checked that differential equations and solutions of the models are at least correct)\nThe main mathematical tool that we use today is the following theorem.\nTheorem 1: Let \\(f: \\mathbb{R}\\to \\mathbb{R}\\) be a continuous function. The \\(C^1\\)-solutions (solutions that have continuous derivative) of differential equation \\(\\frac{d}{dt} y(t) = f(t)\\) are in form of \\(y(t) = \\int_{t_0}^t f(s) \\, ds + C\\) where \\(C\\) is a number in \\(\\mathbb{R}\\).\nProof: By the Fundamental Theorem of Calculus we know that for the function \\(F(t) = \\int_{t_0}^t f(s) \\, ds\\) we have \\(\\frac{d}{dt} F(t) = f(t)\\). This shows one part of the theorem.\nAssume that there are two functions \\(y_1(t), y_2(t)\\) that satisfy the differential equation \\(\\frac{d}{dt} y(t) = f(t)\\). We can calculate that \\(\\frac{d}{dt}(y_1(t)- y_2(t)) = f(t) - f(t) = 0\\). So we are studying the solutions of \\(\\frac{d}{dt} u(t) = 0\\). Only \\(C^1\\)-solutions of that equations are the constant functions \\(u(t) = C\\) where \\(C \\in \\mathbb{R}\\). This can be seen by assuming the opposite and then using the mean value theorem to find a point where \\(\\frac{d}{dt}u(t) \\neq 0\\). So we can conclude that every \\(C^1\\)-solution is form of \\(y(t) = \\int_{t_0}^t f(s) \\, ds + C\\) where \\(C \\in \\mathbb{R}\\).\n\nPopulation models\nIn general, population change of some species in certain area will be determined by following rule\n\\[    \\mathrm{population ~ change} = \\mathrm{births} - \\mathrm{deaths} + \\mathrm{migration}. \\]\nThis is the only way that population size can change. We can model populations with continuous or discrete functions or combination of those. We start by studying continuous population models that do not have a delay factors, i.e. population change is determined by population at current time.\n\n\nMalthusian population growth model\nThe simplest model assumes that the births and the deaths of population is linearly dependent on the population \\(P(t)\\), so \\(\\mathrm{births} = b \\cdot P(t)\\) and deaths \\(\\mathrm{deaths} = d \\cdot P(t)\\). Thus the population dynamics is given by\n\\[\\frac{d}{dt}P(t) = (b-d)P(t), ~~ P(0)=P_0\\]\nThis can be solved the following way (we will denote \\(\\frac{d}{dt} P(t)\\) by \\(P'(t)\\) to make things easier to write.):\n\\[\\begin{equation}\n\\begin{split}\n    & P'(t) = (b-d) P(t) \\\\\n\\iff & \\frac{P'(t)}{P(t)} = (b-d) \\\\\n\\iff & \\frac{d}{dt}(\\ln(P(t))) = (b-d) \\\\\n\\iff & \\ln(P(t)) = (b-d)t + C \\\\\n\\iff & P(t) = e^C e^{(b-d)t} = A e^{(b-d)t}\n\\end{split}\n\\end{equation}\\]\nwhere we have used the Theorem 1. The constant \\(A\\) can be determined by setting \\(t= 0\\) to \\(P(t)= A e^{(b-d)}t\\) and thus obtaining \\(A = P_0\\).\nThe population that follows Malthusian growth model either growths indefinitely and rapidly or decreases to 0 quickly. The infinite growth isn’t very realistic since resources would run out. In the logistic model, there is a way to model the environmental capacity.\n\n\nLogistic growth model\nThe logistic growth model is the following dynamical law\n\\[\\begin{equation}\n    \\label{continuous_logistic_model}\n    N'(t) = r N(t) (1-\\frac{N(t)}{K}), ~~ N(0) = N_0\n\\end{equation}\\]\nwhere \\(K\\) is called environmental capacity of the system and the \\(r\\) is a constant that determines growth rate in Malthusian sense. At small population sizes the model will behave like Malthusian growth model, but when the population grows the factor \\((1- \\frac{N(t)}{K})\\) will make the growth slower and slower. If the population start beyond environmental capacity, the factor \\((1- \\frac{N(t)}{K})\\) will be negative and the population will decrease. When population is at the environmental capacity \\(P(t) = K\\) the population will be stable (and moreover, that point is an attractor of the system).\nThe solution of logistic model is the following\n\\[\\begin{equation}\n    \\label{continuous_logistic_solution}\n  N(t) = \\frac{N_0 K e^{rt}}{K+ N_0 (e^{rt} -1)}.   \n\\end{equation}\\]\nLet’s show this and start by rewriting the differential equation as\n\\[\\begin{equation}\n    \\frac{K N'(t)}{N(t)(K - N(t))} = r.\n\\end{equation}\\]\nIf we can find a partial fraction decomposition in form of\n\\[\\begin{equation}\n    KN'(t)(\\frac{A}{N(t)} + \\frac{B}{K-N(t)})\n\\end{equation}\\]\nfor the left hand side, we obtain something that we can integrate. By doing standard partial fraction calculations, we obtain that \\(A=B=\\frac{1}{K}\\), so the equation takes the form\n\\[\\begin{equation}\n    \\frac{N'(t)}{N(t)} + \\frac{N'(t)}{K-N(t)} = r.\n\\end{equation}\\]\nWe observe that this is equal to\n\\[\\begin{equation}\n    \\frac{d}{dt}(\\ln N(t) - \\ln (K-N(t))) = r.\n\\end{equation}\\]\nBy using the Theorem 1 and rules of logarithms, we get\n\\[ \\ln (\\frac{N(t)}{K-N(t)}) = rt + C \\] so with \\(A = e^C\\)\n\\[\\begin{equation}\n    \\label{eq_for_A_logistic}\n    \\frac{N(t)}{K-N(t)} = A e^{rt}\n\\end{equation}\\]\nand thus\n\\[\\begin{equation}\n    N(t) = \\frac{AK e^{rt}}{1+A e^{rt}}.\n\\end{equation}\\]\nThis looks different from the claim, but when we use initial condition and the equation \\(\\frac{N(t)}{K-N(t)} = A e^{rt}\\) to solve A, we obtain\n\\[\\begin{equation}\n    N(t) = \\frac{KN_0 e^{rt}}{(K-N_0) (1+ \\frac{N_0}{K-N_0} e^{rt})} = \\frac{KN_0 e^{rt}}{(K - N_0 + N_0 e^{rt})} =  \\frac{KN_0 e^{rt}}{K + N_0 (e^{rt} - 1)}\n\\end{equation}\\]\nwhich is same as what we claimed.\n\n\nGompertz growth model\nThere are an another way to modify the logistic population model. In the logistic model, the damping factor, \\((1-N/K)\\) is linearly dependent on \\(N(t)\\). In Gompertz model, the damping factor is chosen to be logarithmically dependent \\(\\ln(\\frac{K}{N(t)}) = \\ln(K) - \\ln(N(t))\\). The model will follow exponential growth longer than logistic model. The evolution equation of the Gompertz model is the following\n\\[\\begin{equation}\n    N'(t) = r N(t)\\ln(\\frac{K}{N(t)}) , ~~ N(0) = N_0.\n\\end{equation}\\]\nThis can be solved analytically. So let’s do that. Let’s us start by writing equation as follows\n\\[\\begin{equation}\n    \\frac{\\frac{N'(t)}{N(t)}}{ \\ln(K) - \\ln (N(t))} =r.\n\\end{equation}\\]\nThis can be seen as a following derivative\n\\[\\begin{equation}\n    -\\frac{d}{dt}\\ln (\\ln (K) - \\ln (N(t)))) = r.\n\\end{equation}\\]\nIn same way as previously, we apply the Theorem 1 to get\n\\[\\begin{equation}\n    \\ln (\\ln(K) - \\ln(N(t))) = -rt + C.\n\\end{equation}\\]\nWhen we apply exponential function to this and move terms, we get\n\\[\\begin{equation}\n    \\label{Gompertz_D}\n    \\ln(N(t)) = \\ln(K) - e^{-rt + C}\n\\end{equation}\\]\nand thus\n\\[\\begin{equation}\n    N(t) = K e^{- e^Ce^{-rt}}.\n\\end{equation}\\]\nThe \\(e^C\\) can be calculated using initial value and using the equation \\(\\ln(N(t)) = \\ln(K) - e^{-rt + C}\\) and obtaining \\(e^C = \\ln(\\frac{K}{N_0})\\). So the solution is\n\\[\\begin{equation}\n    N(t) = K e^{-\\ln(\\frac{K}{N_0})e^{-rt}}.\n\\end{equation}\\]\nThis model converges very quickly to the environmental capacity level."
  },
  {
    "objectID": "posts/Pearl_removal_process_problem/index.html",
    "href": "posts/Pearl_removal_process_problem/index.html",
    "title": "Pearl removal process problem",
    "section": "",
    "text": "One of my friends told me about following problem: Let’s study the following pearl removal process: There are \\(N\\) pearls that forms a circle. Every pearl is numbered \\(1,2,\\dots, N\\) (given a tag) starting from some pearl. Starting from first pearl remove the next pearl. Go to the next pearl that isn’t removed and remove the pearl after that. Repeat this process until there is only one pearl left.\nQuestion: If there are \\(N\\) pearls and the above removal process is applied, what pearl is the last one remaining (what is its tag)?\nHere is how I solved that problem. The solution technique came into my mind from times when I learned about Fast Fourier Transforms and Cooley-Tukey algorithm.\nWe can study this process in rounds: At start of the process we have list of pearls \\(L_0 = (1,2,3,\\dots,N)\\). After each round, we can give a pearl a new tag and we get a new starting list. The list \\(L_k = (1,2,3,\\dots,N_k)\\) means the numbering of the pearls after \\(k\\) laps. At each round, the even numbered pearls are removed. Furthermore, if the number of pearls is odd, the first pearl is removed as well. In even case, the first pearl continues to the next list and start the process again.\nWe can write odd numbers as \\(2k-1\\) for some \\(k\\) (we have used minus sign instead of plus sign to get \\(k=1\\) to match with \\(1\\)). In the case when there are even number of pearls, the pearl’s \\(2k-1\\) new tag will be \\(k\\) and to the other direction: The previous tag of pearl \\(k\\) is \\(2k-1\\). In the odd case, the pearl’s \\(2k-1\\) new tag is \\(k-1\\) (the numbering is shifted by -1 since the first pearl is removed). The previous tag of pearl’s \\(k\\) is \\(2(k+1)-1 = 2k+1\\). Let’s define two functions:\n\\[\\begin{eqnarray}\n    \\label{bw_fn}\n    e(k) &= 2k-1 \\\\\n    o(k) &= 2k+1\n\\end{eqnarray}\\]\nWe need these functions later. Now we can study the length of removal process.\nLemma 1: Let’s write N in the binary\n\\[\\begin{equation}\n    N = \\sum_{i=0}^{m} a_i 2^i\n\\end{equation}\\] where \\(a_i \\in \\{0,1\\}\\) and \\(a_m=1\\). The length of the removal process is \\(m\\) rounds.\nProof: Let \\(N_k\\) denote the size of \\(L_k\\). In the case when \\(N_k\\) is even, the \\(L_{k+1}\\) has half as many elements, so we have \\(N_{k+1} =\\frac{N_k}{2}\\). In odd case, let \\(N_k = 2L +1\\), we can divide the list into two parts \\((1,2,3,\\dots,2L)\\) and \\((2L+1)\\). We can see that in the first part \\(L\\) pearls are removed and the last pearl removes the first pearl and so \\(N_{k+1} = \\frac{N_k-1}{2}\\). In both cases, the \\(N_{k+1}\\) is obtained by chopping off the first digit of \\(N_k\\) written in binary. Thus we can see that the removal process takes \\(m\\) rounds.\nFrom that proof we can see that \\(a_k\\) determines the parity of \\(N_k\\) and we have \\(N_k \\equiv a_k  \\mod 2\\).\nTo the solve original number tag of last pearl, we can work backwardly. In the end, the pearl have the number tag \\(1\\). Using functions \\(e(x)\\) and \\(o(x)\\), we can calculate the tag \\(T_{k-1}\\) in the list \\(L_{k-1}\\) when we know the tag \\(T_k\\) in the list \\(L_k\\). If \\(N_{k-1}\\) ( i.e. \\(a_{k}\\)) is even, then\n\\[\\begin{equation}\n    T_{k-1} = e(T_k) = 2T_k -1\n\\end{equation}\\]\nand if \\(N_{k-1}\\) ( i.e. \\(a_{k-1}\\)) is odd, then\n\\[\\begin{equation}\n    T_{k-1} = o(T_k) = 2T_k +1\n\\end{equation} \\]\nThis gives the answer to the problem. However we can find a exact formula for \\(T_0\\). To show this, we start by studying special cases. When \\(T_k = 1\\), we have \\(e(T_k) = 1\\) and \\(o(T_k) = 11_2\\) (written in binary). Now assume that \\(T_k = \\sum_{i=1}^{j} b_i 2^i + 1\\) for some \\(j&gt;=1\\). When \\(a_{k-1} = 0\\) (\\(N_{k-1}\\) is even), we have\n\\[\\begin{equation}\n    T_{k-1} = e(T_k) = 2(\\sum_{i=1}^{j} b_i 2^i + 1) - 1 =  \\sum_{i=1}^{j} b_i 2^{i+1} + 1.\n\\end{equation}\\]\nFrom this form we can deduce that when \\(a_{k-1}=0\\) we have simply added a zero next between the first and the second digit of \\(T_k\\) when written in binary.\nWhen \\(a_{k-1}\\) (\\(N_{k-1}\\)) is odd, we can write the following\n\\[\\begin{equation}\n    T_{k-1} = o(T_k) = 2(\\sum_{i=1}^{j} b_i 2^i + 1) +1 = \\sum_{i=1}^{j} b_i 2^{i+1} + 2 + 1.\n\\end{equation}\\]\nSo when \\(a_{k-1}=1\\), the function \\(o(x)\\) adds a number one between the first and the second digit of \\(T_k\\) when written in binary. Both functions behaves similarly, they add the \\(a_{k-1}\\) between first and the second digit of \\(T_k\\).\nNow we can discuss how we obtain \\(T_0\\). Recall that we denoted \\(N = \\sum_{i=0}^{m} a_i 2^i\\) When we work backwards, we notice that if \\(a_{m-1}\\ = 0\\) then \\(T_{m-1} = T_m = 1\\). Similarly for each zero before that until we bump into some \\(a_s\\) which is \\(1\\). Formally we can define \\(s\\) as \\(\\max \\{ i ~ |~ a_i = 1, i &lt; m \\}\\), if there is not such \\(s\\) then we obtain that \\(T_0 = 1\\). From above discussion and with knowledge that \\(o(1) = 11_2\\), we can deduce that we add the rest of the binary digits to the left side of the digit 1 starting from \\(a_s\\). All in all, we can deduce that\n\\[\\begin{equation}\n    T_0  = 2 \\sum_{i=0}^{s} a_i 2^i + 1  \n\\end{equation}\\]\nwhere \\(s = \\max (\\{ i ~|~ a_i = 1, i &lt; m \\} \\cup \\{0\\})\\). This can be written also in the following form:\n\\[T_0 = 2 (\\sum_{i=0}^{m} a_i 2^i - 2^m) +1\\]\nThere are also other ways to solve this problem (at least one other method)."
  },
  {
    "objectID": "posts/Bayes_theorem_application/index.html",
    "href": "posts/Bayes_theorem_application/index.html",
    "title": "Bayes’ Theorem says that you are unlikely to have that rare disease that you googled",
    "section": "",
    "text": "I guess that everyone of us have sometimes googled their symptoms and found that they match to some horrible rare disease and then thought “I am screwed”. Now assuming that there is a disease that is lot more common with similar symptoms and those symptoms show up with similar probability, then Bayes’ Theorem says that you are unlikely to have that rare disease. Of course the probability is nonzero so if one is unlucky then one may have it, but the odds are often good that one doesn’t have the disease. I will write shortly about probabilty spaces, basic conditional probability, Bayes’ Theorem and its application to situation above.\n\nBasic definition of a probability space\nA probability space is a triple \\((\\Omega, \\mathcal{F},\\mathbb{P})\\). The set \\(\\Omega\\) is called sample space, \\(\\mathcal{F}\\) is a sigma-algebra whose elements are often called events and \\(\\mathbb{P}\\) is a probability measure. The detailed explanation what these are and what are their properties requires many pages of measure theory and we are not doing it here now, but here is a “short” version what these are.\nWhen we study or model situations where is a discrete number of outcomes (or states of a system), then the sample space \\(\\Omega\\) is a set that contains those outcomes. For example, if we study a dice roll, there are six outcomes that can happen \\(1\\),\\(2\\),\\(3\\),\\(4\\),\\(5\\) and \\(6\\). Then we can take the sample space \\(\\Omega\\) to be the set \\(\\{1,2,3,4,5,6\\}\\). Similarly if we have countably infinite amount of outcomes, then the sample space’s elements are those outcomes.\nWhen the situation’s number of outcomes (or states of the system) is uncountably infinite, it may be not even appropriate to try form explictly the sample space. For example if we are modelling temperature of a room, we don’t want to try to specify what an element of \\(\\Omega\\) looks like or what is contained in \\(\\Omega\\). Often, we just write “let \\(\\omega \\in \\Omega\\)” and continue from that (in those cases, we are more interested in the distributions of random variables which can be interpreted). Sometimes there is a natural model of \\(\\Omega\\) in uncountably infinite case. For example when studying game of darts, a dartboard can be modeled as an union of a two dimensional disk and one point that represents misses (I guess that the latter detail tells everything about my darts skills). Even in this case, if we assume the tip of the dart to hit a single point in the board, the probability to hit certain point in the disk is zero. So it doesn’t make sense to study elements of sample space \\(\\Omega\\) individually (with the exception of that one additional point that represent misses).\nA probability measure is a function \\(\\mathbb{P}: \\mathcal{F} \\to [0,1]\\) with properties that we require it to have:\n\n\\(\\mathbb{P}(\\Omega) = 1\\).\nIf we have countably many sets \\(A_n \\in \\mathcal{F}\\) that are disjoint, then the following holds \\(\\mathbb{P}(\\bigcup_{n=1}^\\infty A_n) = \\sum\\limits_{n=1}^\\infty \\mathbb{P}(A_n)\\). So if we calculate the probability of union of countably many disjoint sets, then the result is a sum of individual probabilities.\n\nSo a probability measure takes a set \\(A\\) (that belongs to \\(\\mathcal{F}\\)) and gives it a number between \\(0\\) and \\(1\\) which we call the probability of event \\(A\\) and denote it by \\(\\mathbb{P}(A)\\). The probability measure will behave nicely with disjoint sets (events) and the probability of whole sample space \\(\\Omega\\) is \\(1\\).\nA sigma-algebra \\(\\mathcal{F}\\) is a collection of subsets of \\(\\Omega\\) that has following three properties:\n\n\\(\\Omega \\in \\mathcal{F}\\)\nIf \\(A\\) belongs to \\(\\mathcal{F}\\), then its compliment \\(A^c\\) belongs also to \\(\\mathcal{F}\\), so \\(A \\in \\mathcal{F} \\implies A^c \\in \\mathcal{F}\\).\nCountable union of sets of \\(\\mathcal{F}\\) is still in \\(\\mathcal{F}\\). If \\(A_n \\in \\mathcal{F}\\) for \\(n \\in \\mathbb{N}\\), then \\(\\bigcup_{n=1}^\\infty A_n \\in \\mathcal{F}\\).\n\nOne can think sigma-algebra as a collection of events that we may observe or measure from the situation and the collection of sets have mathematical properties that make things to work from measure theoretic point of view and make measure theorists almost surely happy (hopefully). When doing probability theory in countable sample spaces, we can select the power set \\(\\mathcal{P}(\\Omega)\\) (the collection of every subset of \\(\\Omega\\)) to be our sigma-algebra, but when we are dealing with uncountable sample spaces, we can’t use the power set since it would break things from measure theoretic point of view.\n\n\nBasic conditional probability\nImagine that we are in a treasure island and we are given a map \\(M \\subset \\mathbb{R}^2\\) of the island. We have thought where the treasure might be and formed a probability measure \\(\\mathbb{P}\\) (and suitable sigma-algebra \\(\\mathcal{F}\\)) that represent our guesses (in practice this would be written via a probability density function). We start our search and have concluded that the treasure can’t be anywhere but in the subset \\(B \\in \\mathcal{F}\\) of the map.\nHow could we update our probability measure function about our believes? The easiest way to update our believe is to form a new probability space with the set \\(B\\) as the building block. So \\(B\\) is our new sample space and we can form a new sigma-algebra by taking intersection of every set of \\(\\mathcal{F}\\) with \\(B\\), so the new sigma-algebra \\(\\tilde{\\mathcal{F}}\\) is given by \\(\\tilde{\\mathcal{F}} = \\mathcal{F} \\cap B\\). We can form a new probability measure \\(\\tilde{\\mathbb{P}}\\) by same trick \\(\\tilde{\\mathbb{P}}(A) = \\mathbb{P}(A \\cap B)\\). Everything is fine now, right??? Oh hell no! Remember the stuff that we wanted our probability measure to fulfill? One of those requirement was the condition \\(\\mathbb{P}(\\Omega) = 1\\). For the new probability measure we have \\(\\tilde{\\mathbb{P}}(B) = \\mathbb{P}(B \\cap B) = \\mathbb{P}(B)\\) which isn’t always \\(1\\). This is a problem, but we can solve this by normalization. We divide the expression by \\(\\mathbb{P}(B)\\) and then everything works. This kind of probability is called conditional probability and is denoted usually as follows\n\\[ \\mathbb{P}(A|B) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)} \\]\nand we assume that we have \\(\\mathbb{P}(B) \\neq 0\\) to this to be well defined. This gives the probabilty of event \\(A\\) under the condition that we know that random event belongs to the set \\(B\\). Notice that conditional probability only scales the probability measure by a constant and thus geometric/proportional properties of probability measure within the set \\(B\\) are almost the same (geometry of probability density function doesn’t change). There are more advanced conditional probability concepts as well, but we are not hopping into them, now.\n\n\nLaw of Total Probability\nTo another concept, a partition of a set. A partition of a set \\(X\\) is a collection \\(\\mathcal{C}\\) of nonempty sets of \\(X\\) with following two properties: Sets are disjoint, so when \\(A,B \\in \\mathcal{C}\\) and \\(A \\neq B\\) we have \\(A \\cap B = \\emptyset\\). Secondly the set \\(X\\) is a union of the sets in the collection, so \\(X = \\bigcup_{A \\in \\mathcal{C}} A\\).\nWe are usually interested in finite partitions of sets or something as easy to handle. Assume now that we have a partition of a sample space \\(\\Omega\\) into finite number of sets \\(A_1,A_2,\\dots, A_n\\). Now we can calculate probability of \\(B\\) with help of that partition as follows\n\\[ \\mathbb{P}(B) = \\mathbb{P}(B \\cap \\Omega) = \\mathbb{P}(B \\cap (\\bigcup_{k=1}^n A_k)) = \\mathbb{P}(\\bigcup_{k=1}^n (B \\cap A_k)) = \\sum_{k=1}^n \\mathbb{P}(B \\cap A_k). \\]\nThe last equality came from the fact that the sets \\(A_k\\) are disjoint. When we notice that we can calculate the probability of intersection of two sets with conditional probability as\n\\[\\mathbb{P}(A \\cap B) = \\mathbb{P}(B |A)\\mathbb{P}(A) = \\mathbb{P}(A|B)\\mathbb{P}(B)\\]\nwe achieve following formula for probability of \\(B\\)\n\\[ \\mathbb{P}(B) = \\sum_{k=1}^n \\mathbb{P}(B|A_k)\\mathbb{P}(A_k)\\].\nThis formula is called the Law of Total Probability.\n\n\nBayes’ Theorem\nAssume that we have a partition of the sample space \\(\\Omega\\) with sets \\(A_1,A_2,\\dots, A_n\\). If we know that random event is in the set \\(B\\), we can ask what is the probability that it belongs to set \\(A_k\\) of the partition, so what is \\(\\mathbb{P}(A_k | B)\\). By using the definition of conditional probability to that and writing the probability \\(\\mathbb{P}(A_k \\cap B)\\) with help of conditional probability, we achieve following equation\n\\[ \\mathbb{P}(A_k | B) = \\frac{\\mathbb{P}(A_k \\cap B)}{\\mathbb{P}(B)} = \\frac{\\mathbb{P}(B|A_k)\\mathbb{P}(A_k)}{\\mathbb{P}(B)}.\\]\nWhen we use the Law of Total Probability to the denominator, we get following formula\n\\[ \\mathbb{P}(A_k | B) = \\frac{\\mathbb{P}(B|A_k)\\mathbb{P}(A_k)}{\\sum_{i=1}^n \\mathbb{P}(B|A_i)\\mathbb{P}(A_i)}. \\]\nwhich is the Bayes’ formula. With it, we can calculate conditional probabilities more easily if the probabilities \\(\\mathbb{P}(B|A_k)\\) and \\(\\mathbb{P}(A_k)\\) are easier to calculate, deduce or estimate.\n\n\nApplication of Bayes’ Theorem to “I am screwed” google diagnosis\nAssume that one has symptoms \\(S\\) and there is a disease \\(D_1\\) which is quite common and some disease \\(D_2\\) which is quite rare. Furthermore, assume that both diseases produces symptoms \\(S\\) with similar probabilities so \\(\\frac{\\mathbb{P}(S|D_1)}{ \\mathbb{P}(S|D_2)}  \\approx 1\\). We can calculate the probability of having disease \\(D_2\\) when symptoms \\(S\\) are present with Bayes’ formula\n\\[ \\mathbb{P}(D_2 |S) = \\frac{\\mathbb{P}(S|D_2)\\mathbb{P}(D_2)}{ \\mathbb{P}(S|D_1)\\mathbb{P}(D_1) + \\mathbb{P}(S|D_2)\\mathbb{P}(D_2)}.\\]\nwhen we use elementary trick \\(\\frac{a}{b} = \\frac{1}{\\frac{b}{a}}\\) we obtain the following approximation\n\\[ \\mathbb{P}(D_2 |S) = \\frac{1}{\\frac{\\mathbb{P}(S|D_1)\\mathbb{P}(D_1)}{\\mathbb{P}(S|D_2)\\mathbb{P}(D_2)} + 1 } \\approx \\frac{1}{ \\frac{\\mathbb{P}(D_1)}{\\mathbb{P}(D_2)} + 1}\\]\nThe quantity \\(\\mathbb{P}(D)\\) is called prevalence of disease and it tells how many are affected by the disease at particular point of time (the proportion of the affected population with respect to whole population). For example if the scale of prevalence of disease \\(D_1\\) is \\(10^{-1}\\) and the scale of prevalence of disease \\(D_2\\) is \\(10^{-4}\\) then our approximation gives an estimate\n\\[ \\mathbb{P}(D_2| S) \\approx \\frac{1}{1 + \\frac{10^{-1}}{10^{-4}}} = \\frac{1}{1001}\\]\nfor the probability. Which is a small probability with compared to “I am screwed” probability. This is only a way to estimate probabilities and should not be used as a conclusive deduction tool whether one has a disease or not.\nMany national healthcare providers have nowadays good websites that provide good information about symptoms/diseases and when to contact healthcare providers and what self-treatment options are there for something. If you are unsure if something needs medical attention, you can always contact your healthcare provider and ask if those symptoms require attention of a medical professional. And of course in the case of medical emergency, the first thing to do is to contact the emergency medical services.\nI hope that you (re)learnt something about mathematics from this and this helps with stress while reading about symptoms from the internet or somewhere else. I also hope that you stay healthy!\n\nEdit 1 (November 13, 2025): Fixed some typos, grammar and wording."
  },
  {
    "objectID": "posts/Finding_multiple_mathematical_models_for_same_phenomenon_is_a_good_thing/index.html",
    "href": "posts/Finding_multiple_mathematical_models_for_same_phenomenon_is_a_good_thing/index.html",
    "title": "Finding multiple mathematical models for same phenomenon is a good thing",
    "section": "",
    "text": "Finding an unifying collection of equations and principles that would explain everything in the universe under single compatible framework would be a holy grail for physicists and mathematicians. With theory of everything, you could potentially predict everything which could be very useful. Let’s say that some day such theory is found. Are other models unneccesary after that? I would like to argue that knowing and finding other mathematical models is important even in that scenario (as it is useful to know/figure out multiple solutions for a problems and multiple proofs for a single theorem). I hope that with this I could show how to analyze mathematical models from human perspective: What would make mathematical models (or theories) easier or harder humans to learn/understand.\n\n“Mechanistical” model of a system.\nThere are many kind of mathematical models, but now I want discuss “mechanistical” models of dynamical systems. The following is one way to think about dynamical systems and the world: Mathematical model of dynamical system consist of environment/space \\(\\mathcal{E}\\), building blocks \\(\\mathcal{B}\\) and dynamics between building blocks \\(\\mathcal{D}\\) and a mathematical model of a system could be thought as a triple \\((\\mathcal{E},\\mathcal{B}, \\mathcal{D})\\). Additionally, when studying a system that evolve, the history of the system could be an important part of the model as it would describe why there are certain things in the system (this is one good idea in John H. Holland’s book Signals and boundaries).\nThe environment part of the model will describe the spatial/environmental aspects of a system and how we think that system interacts with outside world. For example, boundary conditions describes often how system interacts with outside world. Building blocks describe smallest parts of the system that we want to include to the model. For example building blocks of system might be particles, field quantities or agents. Dynamics of the system tells how we think that the building blocks of a system will interact with each other.\nMany different models can describe a system equivalently or nearly equivalently and there isn’t an unique model for a system. This can be seen by a thought experiment: Let’s have a model that describe system to a desired precision. If we have another model that has smaller building blocks than previous model and we can build the bigger building blocks from smaller ones, then the models would produce nearly equivalent results in the scope of the model that has bigger building blocks. (Assuming that both models produce accurate results and models are applied in the scale where they are precise).\nIn a way, we notice that some mathematical models form a hierarchial/nested structure. In mathematical modelling there is at least two questions: What is the smallest building block in the model and what part or pheomenon of a system we want to understand or be able to predict. Sometimes, we don’t need to understand or model every part of a system or what happens between events and sometimes we need to: If we want to predict where a golf ball lands after a swing, we don’t necessarily need to model the dynamics of atmosphere, but if we study where a frisbee lands, then we may need to, at least in some accuracy.\nThis is true also while writing mathematics and proofs. We select what axioms we take for granted, what theorems and properties we assume to be known and what objects are already defined. Sometimes unpacking a proof with details can add clarity as it makes mental steps smaller. Sometimes it only confuses the reader as the journey takes longer and is messier. The above characterization can be little bit non-exact. Let me try to somehow formulate and “quantify” what I mean.\n\n\nTradeoffs of mathematical models and human limits.\nIn every model, we have certain number of\n\nobjects or building blocks,\ndynamical laws and principles,\ncomputational steps (computational load).\n\nWhen we look from human perspective, humans need to be capable to\n\nremember,\nunderstand,\nbuild and\napply\n\nthe model at some level if they want to get the joy of understanding. One of the most limiting factors of humans is memory capabilities. If we want to achieve intellectually elegant model, finding a model that minimizes the amount of building blocks, dynamics and computational load of the model, would the best bet. Of course, without sacrificing the precision and the prediction capabilities of the model.\nIf we don’t care how enjoyable the model is and how much human understanding it provides about dynamical laws of the system, we can totally forget about minimizing the model since computer memory is cheap and we can go ham with it. For example, some AI models tend to be black box models (they work well, but we don’t know how and why they work).\nFor both, the selection of smallest building blocks determines the resolution of model: Often we can’t get results or understanding about phenomena or dynamics of a systems that are smaller than our smallest building block. If the building block are selected to be too small, achieving knowledge about much larger pheomena or dynamics may be computationally too demanding or slow even for the best computer.\nWhen we return to think about a model that gives a theory of everything. I would guess that its smallest building blocks would be really small and thus computing anything about human size phenomena would be infeasible. Therefore, we need to continue to develop and learn different mathematical models. When learning or studying something, changing what one takes to be smallest building blocks can be really enlightening and make the journey easier."
  },
  {
    "objectID": "posts/EV_doesnt_always_tell_whole_story/index.html",
    "href": "posts/EV_doesnt_always_tell_whole_story/index.html",
    "title": "Expected value does not always tell the whole story",
    "section": "",
    "text": "In poker, there is a rule of thumb that you should play plus EV (Expected Value) lines and avoid negative EV lines and indeed it is important to play more plus EV lines than negative EV lines in the long run. However, expected value is a mathematical “measurement” that has its limitations and can be misleading sometimes. It would be really weird if just one number would be able to provide perfect information for decision making.\nThere are examples of cases where making plus EV decisions would be absolutely insane. Those cases are quite simple, but it can be instructive to see how one would construct such scenarios. Sometimes mathematics is all about building examples where things doesn’t work as one would imagine.\n\nBasics of expected value\nHow one could end up with inventing expected value: Let’s start by considering a scenario that has two outcomes: The outcome \\(1\\) has probability \\(p_1\\) and a result \\(r_1.\\) The outcome \\(2\\) has probability \\(p_2\\) and a result \\(r_2.\\) How could we evaluate the scenario? Let start by two observations:\n\nIf the result \\(r_1\\) increases, then the evaluation should increase. The opposite should also be true.\nIf the probability of \\(p_1\\) increases, the effect (weight) of the result should increase and if the probability goes to zero, the effect of that result should vanish.\n\nBy multiplying \\(p_1\\) and \\(r_1\\) we get an object \\(p_1 r_1\\) that has above desired behaviour with respect of the first outcome. We can do this as well for the other outcome as well and get \\(p_2 r_2\\). By summing these two, we get some way to evaluate the result \\(R\\) of the scenario: \\[\\mathbb{E}(R) = p_1 r_1 + p_2 r_2.\\] This is called expected value of \\(R\\). This can be generalized to the case when we have several outcomes. Let’s us have a scenario that has \\(N\\) outcomes and outcome \\(k\\) has probabilty \\(p_k\\) and a result \\(r_k\\), then the expected value of the result \\(R\\) of the scenario is \\[ \\mathbb{E}(R) = \\sum_{k=1}^N p_k r_k.\\] Each component of that sum has the desired behavior that we listed above. Furthermore, the expected value \\(\\mathbb{E}(R)\\) as a whole has the property that if some probabilty \\(p_k = 1\\), then \\(\\mathbb{E}(R) = r_k\\) which is the behaviour that we would want from the evaluation (If we know that event \\(k\\) is going to happen for sure, then the evaluation should be exactly the result \\(r_k\\)). Thus, the expected value \\(\\mathbb{E}(R)\\) has a sensible behaviour. EV can be generalized to the case when there are (countably) infinitely many discrete cases and to the case when the random variable (result) is continuous, vector-valued or something even wilder.\n\n\n\n\n\n\nNoteAn elementary property of expected value in the discrete finite case\n\n\n\nExpected value is always between the smallest and largest value of the result (random variable). Constructing a proof (mathematical argument) for this is a great exercise if you haven’t made many proofs in your life. This property is also something that makes sense and is something that we would like to have.\n\n\nNow, we can give scenarios some kind of evaluation about what to expect. One could think if EV is a good number in the context of scenario, then the scenario would be good, but this isn’t always the case.\n\n\nHow could we make an example where EV behaves unintuively?\nIn mathematics, one way to break things is to choose really large or really small numbers. We need to do both to produce an example. Let’s return to the scenario where we had two outcomes and set \\(r_1 = -10\\) (An entry fee of 10 units of money to a game which is lost if one loses). Let’s choose the prize of the game to be absolutely bonkers \\(r_2 = 10^{20}\\) units of money and the probability of winning to be \\(p_2=10^{-10}\\). Then the EV is\n\\[ \\mathbb{E}(R) = (1-10^{-10}) \\cdot (-10) + 10^{-10} \\cdot 10^{20} =  10^{10} -(1-10^{-10}) \\cdot 10  \\approx 10^{10}. \\]\nEV of approximately \\(10^{10}\\) units of money is a decent amount money in any currency even in this current cost of living crisis. That is a good EV in my books and we could tinker it to be big as we want. But the problem is that probability of winning is so small that one would never win in that game, on average.\nOut of the curiousness, let’s calculate how many tries it would take to win in that game, on average: The probability that we would win at try \\(k\\) is \\(\\mathbb{P}(Q=k) = (1-p)^{k-1}p\\) (we lose \\(k-1\\) times and then win). Now we take a mathematical shortcut and use known results instead of calculating by hand: The random variable \\(Q\\) has a geometric distribution which has expected value of \\(\\mathbb{E}(Q) = \\frac{1}{p}\\). So it would take \\(\\frac{1}{10^{-10}}=10^{10}\\) tries, on average. A person that lives 100 years has about \\(3.1536 \\cdot 10^9\\) seconds in their lives and that wouldn’t be enough if they played the game every second, on average.\n\n\n\n\n\n\nNotesmall differences in luck can make huge difference in life.\n\n\n\nFrom that math above: Let’s say that you have \\(1\\%\\) chance to get a job, you need to make 100 quality applications, on average. If you can improve that by just \\(1\\%\\), the average amount of number of applications needed decreases by 50. We can deduce that when things are hard, even small improvements can make a huge difference.\nOne can also see that even small differences in general luck between humans can lead to drastically different experience of life. Let’s say that person \\(1\\) has general luck of \\(1\\%\\) and person \\(2\\) has general luck of \\(5\\%\\), the difference in luck is only \\(4\\%\\), but person \\(2\\) has to make \\(80\\%\\) less tries, on average.\nOf course, these observations are only relevant in the situations when the probabilities are small.\n\n\nI know that it is a bit funny to say beware of EV and then use the EV of geometric distribution to show why you need to be wary about EVs. The phrase “on average” has been doing the most of the heavy lifting during this blog post. What do we mean by “on average” and how we could determine how attainable the EV is?\n\n\nThe central limit theorem\nWhen we run same scenario \\(n\\) times in a way where the runs are independent of each others, then we can study the average result, \\(A_n\\), of the results:\n\\[ A_n  = \\frac{\\sum_{k=1}^n R_k}{n} \\]\nwhere the \\(R_k\\) is the result of the \\(k\\)-th run and every \\(R_k\\) will have the same distribution \\(R\\). Now there exists a very famous theorem that connects the average result, \\(A_n\\), to the expected value \\(\\mathbb{E}(R)\\).\nThe central limit theorem says the following: Assume that \\(R, R_1,R_2, \\dots, R_n\\) are identially independently distributed random variables with finite expected value \\(\\mathbb{E}(R)\\) and finite variance \\(\\mathrm{Var}(R) = \\sigma^2\\). Then as the \\(n\\) grows, the distribution of \\(\\frac{A_n - \\mathbb{E}(R)}{\\frac{\\sigma}{\\sqrt{n}}}\\) converges in distribution to the normal distribution \\(\\mathcal{N}(0,1)\\) and the same in the limit notation\n\\[ \\lim_{n \\to \\infty} \\frac{A_n - \\mathbb{E}(R)}{\\frac{\\sigma}{\\sqrt{n}}} \\xrightarrow{d} \\mathcal{N}(0, 1). \\]\nHere \\(\\sigma^2\\) is the variance of \\(R\\). Before we continue our journey to definition of variance, “couple” words about what this means. Basically, this means that when \\(n\\) grows, we can approximate probability calculations about the average \\(A_n\\) with the normal distribution \\(\\mathcal{N}(\\mathbb{E}(R), \\frac{\\sigma^2}{n})\\) as the approximation.\n\n\n\n\n\n\nCautionAn instructive silly goose moment No. 1 (a bit advanced exercise)\n\n\n\n\n\nI tried to find/make a formulation of the central limit theorem that would say something like this\n\\[ \\lim_{n \\to \\infty} A_n \\xrightarrow{d} \\mathcal{N}(\\mathbb{E}(R), \\frac{\\sigma^2}{n}) \\]\nas it would show better what happens to \\(A_n\\), but then I realized that I was a really silly goose. Why is finding a formulation like this impossible?\n\n\n\nMoreover, if the probability density function of \\(R\\) for example is bounded, we can say that probability density function of \\(\\frac{A_n - \\mathbb{E}(R)}{\\frac{\\sigma}{\\sqrt{n}}}\\) converges pointwisely to the probability density function of the normal distribution \\(\\mathcal{N}(0,1)\\) (the probability density function of \\(A_n\\) will look like normal distribution at some point). This isn’t true for every random variable that fulfills the central limit theorem conditions, so we need to be delicate how we interpret the central limit theorem. Local limit theorems give conditions when the probability density function of \\(\\frac{A_n - \\mathbb{E}(R)}{\\frac{\\sigma}{\\sqrt{n}}}\\) converges pointwisely to the normal distribution \\(\\mathcal{N}(0,1)\\). Additional information is available for example from here, here or from here. (Not an expert on local limit theorems, so take my words with a grain of salt)\nIf there were terms or stuff that were unfamiliar or confusing, you can skip them and focus on “when n grows, the probability distribution of the average \\(A_n\\) starts to behave like it would be normal distribution \\(\\mathcal{N}(\\mathbb{E}(R), \\frac{\\sigma^2}{n})\\)”. The exact way to handle technical details is a much longer story and we are not delving into it here. The most important observation is that the variance will decrease when \\(n\\) grows. I think that the following interactive visualization is the best way to demonstrate how normal distribution look and how variance \\(\\frac{\\sigma^2}{n}\\) affect it.\n\nfunction square(x){\n\n  return x*x;\n}\n\nfunction range(a, b, h) {\n  if (h &lt;= 0) throw new Error('Step size h must be positive')\n  // Ensure we move in the right direction even if a &gt; b\n  const forward = a &lt;= b\n  const step = forward ? h : -h\n  const result = []\n\n  // Use a tolerance to avoid floating‑point drift (e.g., 0.1 + 0.2 ≠ 0.3 exactly)\n  const eps = Math.abs(step) / 1e12\n\n  for (let v = a; forward ? v &lt;= b + eps : v &gt;= b - eps; v += step) {\n    // Round to a reasonable number of decimal places to hide FP artefacts\n    result.push(Number(v.toFixed(12)))\n  }\n\n  return result\n}\n\nx = range(-100,100,0.1);\n\ny = x.map(x=&gt;normalPDF(x,mu,variance));\n\n\n\n\nfunction normalPDF(x, mu, var_) {\n  const sigma = Math.sqrt(var_);\n  const coefficient = 1 / (sigma * Math.sqrt(2 * Math.PI));\n  const exponent = -Math.pow(x - mu, 2) / (2 * var_);\n  return coefficient * Math.exp(exponent);\n}\n\n\nPlot.plot({\n  marks: [\n    Plot.line(\n      x.map((d, i) =&gt; ({x: d, y: y[i]})),\n      {x: \"x\", y: \"y\"}\n    )\n  ],\n  x: {label: \"X-axis\"},\n  y: {label: \"Y-axis\"}\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof mu = Inputs.range([-30, 30], {label: \"the expected value \", step: 1});\nviewof variance = Inputs.range([0.1, 1000], {label: \"Variance\", step: 0.1});\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo we can say that in the long run, the average result \\(A_n\\) will converge to the expected value \\(\\mathbb{E}(R)\\), i.e.\n\\[ \\lim_{n \\to \\infty} A_n \\rightarrow \\mathbb{E}(R).\\]\nThis holds in sense of “convergence in probability” and “converges almost surely” and these kind of limit theorems are variants of law of large numbers theorems.\nIn general, there isn’t an exact way to tell how long it takes before the average converges to the expected value, but qualitatively we can say that the larger variance is, the larger probability to deviate from the expected value is at the beginning. The same is true for the question “when the normal distribution approximation is good enough?”.\n\n\nSo, what is variance?\nI think that the central limit theorem shows why variance is an important quantity. The variance, \\(\\mathrm{Var}(R)\\), of random result (variable) \\(R\\) is defined as follows\n\\[ \\mathrm{Var}(R) = \\mathbb{E}[(R-\\mathbb{E}(R))^2]. \\]\nLet us write this for the case when we have finite number results \\(r_1,r_2,\\dots, r_N\\) with probabilities \\(p_1,p_2,\\dots p_N\\) respectively:\n\\[ \\mathrm{Var}(R) = \\sum_{k=1}^N p_k(r_k - \\mathbb{E}(R))^2. \\]\nIn a way, variance studies how far away every potential result is from expected value and the distance is weighted by the probability of the result and these quantities are summed together. There are multiple potential choices for “distance function”. For example, one could use \\(|R-\\mathbb{E}(R)|\\) as well, but the square function produces the most useful properties for the variance.\nLet’s calculate the variance in our unintuitive case (using the approximated EV, \\(\\mathbb{E}(R) \\approx 10^{10}\\))\n\\[ \\mathrm{Var}(R) \\approx (1-10^{-10})\\cdot (-10-10^{10})^2 + 10^{-10} \\cdot (10^{20} - 10^{10})^2 \\approx 10^{30}. \\] For some odd reason, I would guess that with this variance the convergence to expected value might take a while.\n\n\n\n\n\n\nNoteYou can also use Chebyshev’s inequality to estimate what is the probability of getting results that are at least certain distance from the expected value\n\n\n\nThe Chebyshev’s inequality says that for a random variable \\(R\\) with \\(\\mathbb{E}(|R|) &lt; \\infty\\) and finite variance the following inequality holds\n\\[ \\mathbb{P}(|R-\\mathbb{E}(R)| \\geq a ) \\leq \\frac{\\mathrm{Var}(R)}{a^2}.\\]\nThis gives a crude way to estimate probabilities, but this gives an upper bound for the probabilities without making assumptions about convergence or other stuff. Also, it is good enough for proving weak law of large numbers.\n\n\nWhat could we learn here? If someone says to you that here is an opportunity with a great EV, the question “what is the variance of the said opportunity?” would be a really good question to figure out. The next good question would be “What is the 95% confidence interval of the said opportunity?”, but that question is another story for another time."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Pearl removal process problem\n\n\n\nProblem solving\n\n\n\n\n\n\n\n\n\nNov 23, 2025\n\n\nVeli-Pekka Uusluoto\n\n\n\n\n\n\n\n\n\n\n\n\nAnalytically solvable single species population models part 1\n\n\n\nMathematical biology\n\n\n\n\n\n\n\n\n\nNov 16, 2025\n\n\nVeli-Pekka Uusluoto\n\n\n\n\n\n\n\n\n\n\n\n\nBayes’ Theorem says that you are unlikely to have that rare disease that you googled\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\nNov 12, 2025\n\n\nVeli-Pekka Uusluoto\n\n\n\n\n\n\n\n\n\n\n\n\nFinding multiple mathematical models for same phenomenon is a good thing\n\n\n\nMathematical modelling\n\n\n\n\n\n\n\n\n\nNov 4, 2025\n\n\nVeli-Pekka Uusluoto\n\n\n\n\n\n\n\n\n\n\n\n\nExpected value does not always tell the whole story\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\nOct 28, 2025\n\n\nVeli-Pekka Uusluoto\n\n\n\n\n\nNo matching items"
  }
]